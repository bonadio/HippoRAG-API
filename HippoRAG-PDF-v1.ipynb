{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of HippoRAG using only APIs to search over a PDF file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDF extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "from uuid import uuid4\n",
    "unique_id = uuid4().hex[0:8]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"HippoRag-API - {unique_id}\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"LANGCHAIN_API_KEY\"  # Update to your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from the PDF\n",
    "\n",
    "pdf_file_path = \"/Users/viewitpro/Downloads/HIDROPONIA.pdf\"\n",
    "# pdf_file_path = \"/Users/viewitpro/Downloads/MANUAL_DUSTER.pdf\"\n",
    "\n",
    "pdf_loader = PyPDFLoader(pdf_file_path)\n",
    "pdf_pages = pdf_loader.load_and_split()\n",
    "pdf_full_text = \"\\n\".join([d.page_content for d in pdf_pages])\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=5000,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "passages_doc_arr = text_splitter.create_documents([pdf_full_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RippoRAG INDEXING \n",
    "- Extract name entities from each passage\n",
    "\n",
    "- Create Triplets\n",
    "- Encode Entities (OpenAI text-embedding-3-small with 256 vector size)\n",
    "- Find similar entities to create E' relations triplets (THIS IS NOT IMPLEMENTED)\n",
    "- Add to GraphDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "\n",
    "llm_model = \"gpt-3.5-turbo-1106\" # 1106 works better extracting more entities\n",
    "\n",
    "# changed the prompt to include \"in the same language as the paragraph\"\n",
    "extract_entities_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"Your task is to extract named entities from the given paragraph, in the same language as the paragraph.\n",
    "Respond with a JSON list of entities.\"\"\"),\n",
    "        (\"human\", \"\"\"Paragraph:\n",
    "```\n",
    "Radio City\n",
    "Radio City is India's first private FM radio station and was started on 3 July 2001.\n",
    "It plays Hindi, English and regional songs.\n",
    "Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features.\n",
    "```\"\"\"),\n",
    "        (\"ai\", \"\"\"{{\"named_entities\":\n",
    "    [\"Radio City\", \"India\", \"3 July 2001\", \"Hindi\", \"English\", \"May 2008\", \"PlanetRadiocity.com\"]\n",
    "}}\"\"\"),\n",
    "        (\"human\", \"\"\"Paragraph:```\n",
    "{passage_text}\n",
    "```\"\"\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "extract_triplets_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"Your task is to construct an RDF (Resource Description Framework) graph from the given passages and named entity lists. \n",
    "Respond with a JSON list of triples, with each triple representing a relationship in the RDF graph. \n",
    "\n",
    "Pay attention to the following requirements:\n",
    "- Each triple should contain at least one, but preferably two, of the named entities in the list for each passage.\n",
    "- Clearly resolve pronouns to their specific names to maintain clarity.\n",
    "\"\"\"),\n",
    "        (\"human\", \"\"\"Convert the paragraph into a JSON dict, it has a named entity list and a triple list.\n",
    "Paragraph:\n",
    "```\n",
    "Radio City\n",
    "Radio City is India's first private FM radio station and was started on 3 July 2001.\n",
    "It plays Hindi, English and regional songs.\n",
    "Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features.\n",
    "```\n",
    "\n",
    "{{\"named_entities\":\n",
    "    [\"Radio City\", \"India\", \"3 July 2001\", \"Hindi\", \"English\", \"May 2008\", \"PlanetRadiocity.com\"]\n",
    "}}\"\"\"),\n",
    "        (\"ai\", \"\"\"{{\"triples\": [\n",
    "            [\"Radio City\", \"located in\", \"India\"],\n",
    "            [\"Radio City\", \"is\", \"private FM radio station\"],\n",
    "            [\"Radio City\", \"started on\", \"3 July 2001\"],\n",
    "            [\"Radio City\", \"plays songs in\", \"Hindi\"],\n",
    "            [\"Radio City\", \"plays songs in\", \"English\"]\n",
    "            [\"Radio City\", \"forayed into\", \"New Media\"],\n",
    "            [\"Radio City\", \"launched\", \"PlanetRadiocity.com\"],\n",
    "            [\"PlanetRadiocity.com\", \"launched in\", \"May 2008\"],\n",
    "            [\"PlanetRadiocity.com\", \"is\", \"music portal\"],\n",
    "            [\"PlanetRadiocity.com\", \"offers\", \"news\"],\n",
    "            [\"PlanetRadiocity.com\", \"offers\", \"videos\"],\n",
    "            [\"PlanetRadiocity.com\", \"offers\", \"songs\"]\n",
    "    ]\n",
    "}}\"\"\"),\n",
    "        (\"human\", \"\"\"Convert the paragraph into a JSON dict, it has a named entity list and a triple list.\n",
    "Paragraph:\n",
    "```\n",
    "{passage_text}\n",
    "```\n",
    "\n",
    "{named_entities}\"\"\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# passages_arr = [ { \"id\":idx, \"text\":passage.page_content } for idx, passage in enumerate(passages_doc_arr[:4])]\n",
    "passages_arr = [ { \"id\":idx, \"text\":passage.page_content } for idx, passage in enumerate(passages_doc_arr)]\n",
    "\n",
    "for passage in passages_arr:\n",
    "    json_output_parser = SimpleJsonOutputParser()\n",
    "    chain_entities = extract_entities_prompt | ChatOpenAI(model=llm_model, temperature=0.0) | json_output_parser\n",
    "    named_entities = chain_entities.invoke({\"passage_text\": passage[\"text\"]})\n",
    "    passage[\"named_entities\"] = named_entities[\"named_entities\"]\n",
    "\n",
    "    chain_triples = extract_triplets_prompt | ChatOpenAI(model=llm_model, temperature=0.0) | json_output_parser\n",
    "    triples = chain_triples.invoke({\"passage_text\": passage[\"text\"], \"named_entities\": named_entities})\n",
    "    passage[\"triples\"] = triples[\"triples\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to the passage named_entities name entities created from the triples\n",
    "for passage in passages_arr:\n",
    "    named_entities = passage[\"named_entities\"]\n",
    "    named_entities = [entity.lower() for entity in named_entities]\n",
    "    for triple in passage[\"triples\"]:\n",
    "        if len(triple) != 3:\n",
    "            continue\n",
    "        named_entities.extend([triple[0].lower(), triple[2].lower()])\n",
    "    \n",
    "    passage[\"named_entities\"] = list(set(named_entities)) # remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a unique id for each named entity\n",
    "named_entities_dict = {}\n",
    "named_entities_next_id = 0\n",
    "\n",
    "for passage in passages_arr:\n",
    "    named_entities = passage[\"named_entities\"]\n",
    "    for idx, named_entity in enumerate(named_entities):\n",
    "        named_entities_dict[named_entity] = named_entities_next_id\n",
    "        named_entities_next_id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Entities\n",
    "entities = []\n",
    "for passage in passages_arr:\n",
    "    for entity in passage[\"named_entities\"]:\n",
    "        entities.append(entity)\n",
    "\n",
    "# Embedding Entities using OpenAI\n",
    "openai_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=256)\n",
    "entities_embeddings = openai_embeddings.embed_documents(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update passages_arr with entity embeddings\n",
    "pos = 0\n",
    "for passage in passages_arr:\n",
    "    passage[\"entity_embeddings\"] = []\n",
    "    for _ in passage[\"named_entities\"]:\n",
    "        passage[\"entity_embeddings\"].append(entities_embeddings[pos])\n",
    "        pos += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for passage in passages_arr:\n",
    "#     print(passage[\"named_entities\"])\n",
    "#     print(passage[\"triples\"])\n",
    "#     print(\"\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to GraphDB \n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "URI = os.environ.get(\"NEO4J_URI\")\n",
    "AUTH = (os.environ.get(\"NEO4J_USER\"), os.environ.get(\"NEO4J_PASSWORD\"))\n",
    "\n",
    "def create_triplets(tx, triplets):\n",
    "    query = \"\"\"\n",
    "    UNWIND $triplets AS triplet\n",
    "    MERGE (a:Entity {name: triplet.subject, passageId: triplet.passageId, embedding: triplet.embedding_subject, node_id: triplet.node_id_subject})\n",
    "    MERGE (b:Entity {name: triplet.object, passageId: triplet.passageId, embedding: triplet.embedding_object, node_id: triplet.node_id_object})\n",
    "    MERGE (a)-[:RELATES_TO {type: triplet.predicate}]->(b)\n",
    "    \"\"\"\n",
    "    tx.run(query, triplets=triplets)\n",
    "\n",
    "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "    with driver.session() as session:\n",
    "        triplets = []\n",
    "        for passage in passages_arr:\n",
    "            for triple in passage[\"triples\"]:\n",
    "                if len(triple) != 3:\n",
    "                    continue\n",
    "                # print(triple)\n",
    "                predicate=triple[1].replace(\" \", \"_\").upper()\n",
    "                subject=triple[0].lower()\n",
    "                object=triple[2].lower()\n",
    "                triplets.append({\n",
    "                    \"subject\": subject, \n",
    "                    \"predicate\": predicate, \n",
    "                    \"object\": object,\n",
    "                    \"passageId\": passage[\"id\"],\n",
    "                    \"embedding_subject\": passage[\"entity_embeddings\"][passage[\"named_entities\"].index(subject)],\n",
    "                    \"embedding_object\": passage[\"entity_embeddings\"][passage[\"named_entities\"].index(object)],\n",
    "                    \"node_id_subject\": named_entities_dict[subject],\n",
    "                    \"node_id_object\": named_entities_dict[object]\n",
    "                })\n",
    "\n",
    "        session.execute_write(lambda tx: create_triplets(tx, triplets))\n",
    "\n",
    "        # create the vector index on  Neo4j\n",
    "        def create_vector_index(tx):\n",
    "            query = \"\"\"\n",
    "            CREATE VECTOR INDEX entitieDB IF NOT EXISTS\n",
    "            FOR (m:Entity)\n",
    "            ON m.embedding\n",
    "            OPTIONS {indexConfig: {\n",
    "                `vector.dimensions`: 256,\n",
    "                `vector.similarity_function`: 'cosine'\n",
    "            }}\n",
    "            \"\"\"\n",
    "            tx.run(query)\n",
    "        \n",
    "        session.execute_write(lambda tx: create_vector_index(tx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RippoRAG RETRIEVAL\n",
    "- Extract name entities from query\n",
    "\n",
    "- Vector search on GraphDB to find similar Entities\n",
    "- PPR on GraphDB to find \"good\" nodes\n",
    "- Rank passages from nodes found (THIS IS NOT IMPLEMENTED)\n",
    "- Send passages + query to LLM for answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract name entities from query\n",
    "\n",
    "def extract_entities_from_query(user_query):\n",
    "    # This prompt is a simpler version o the original, it works better for small paragraphs and less entities and\n",
    "    # in other languages like portuguese\n",
    "    extract_entities_custom_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"\"\"Your task is to extract all entities from the given paragraph, in the same language as the paragraph.\n",
    "    Respond with a JSON list of entities like {{\"entities\":[\"entity1\", \"entity2\", ...]}}\"\"\"),\n",
    "            (\"human\", \"\"\"Paragraph:```\n",
    "    {passage_text}\n",
    "    ```\"\"\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    json_output_parser = SimpleJsonOutputParser()\n",
    "    chain_query_entities = extract_entities_custom_prompt | ChatOpenAI(model=llm_model, temperature=0.0) | json_output_parser\n",
    "    #chain_query_entities = extract_entities_prompt | ChatOpenAI(model=llm_model, temperature=0.0) | json_output_parser\n",
    "    query_entities = chain_query_entities.invoke({\"passage_text\": user_query})\n",
    "    query_entities[\"named_entities\"] = query_entities[\"entities\"] # change the name to named_entities\n",
    "\n",
    "    return query_entities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vector search on GraphDB to find similar Entities\n",
    "\n",
    "#https://neo4j.com/docs/cypher-manual/current/indexes/semantic-indexes/vector-indexes/\n",
    "\n",
    "def vector_search_graphdb(query_entitie, min_score=0.8):\n",
    "    openai_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=256)\n",
    "    query_entitie_embedding = openai_embeddings.embed_query(query_entitie)\n",
    "    results = []\n",
    "    with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "        with driver.session() as session:\n",
    "            def search_vector(tx, query_embedding):\n",
    "                cypher_query = f\"\"\"\n",
    "                CALL db.index.vector.queryNodes('entitieDB', 3, {query_embedding}) YIELD node, score\n",
    "                RETURN node.node_id as id, node.name as name, score\n",
    "                \"\"\"\n",
    "                return tx.run(cypher_query).data()\n",
    "\n",
    "            db_result = session.execute_read(lambda tx: search_vector(tx, query_entitie_embedding))\n",
    "            \n",
    "            for result in db_result:\n",
    "                if result[\"score\"] >= min_score:\n",
    "                    results.append(result)\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PPR on GraphDB to find \"good\" nodes\n",
    "\n",
    "# gds.pageRank only works on Neo4j 4.0 auraDS and its a payed feature on Neo4j 4.0 \n",
    "# for this reason I will export the graph from auraDB to igraph and use the igraph implementation of PageRank  \n",
    "import igraph as ig\n",
    "\n",
    "def ppr_graphdb(nodes):\n",
    "\n",
    "    passage_ids = []\n",
    "    with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "        with driver.session() as session:\n",
    "            #get nodes and relations from graphDB\n",
    "            graph_nodes = session.execute_read(lambda tx: tx.run(\"MATCH (n) RETURN n.node_id AS node_id, n.passageId as passageId\").data())\n",
    "            relations = session.execute_read(lambda tx: tx.run(\"MATCH (a)-[r]->(b) RETURN a.node_id AS source, b.node_id AS target\").data())\n",
    "\n",
    "            # Create an igraph graph\n",
    "            g = ig.Graph(directed=True) \n",
    "            # add nodes\n",
    "            for gnode in graph_nodes:\n",
    "                g.add_vertex(name=str(gnode[\"node_id\"]), labels=str(gnode[\"node_id\"]), passageId=gnode[\"passageId\"])\n",
    "\n",
    "            # add edges\n",
    "            g.add_edges([(str(rel[\"source\"]), str(rel[\"target\"])) for rel in relations])\n",
    "\n",
    "            # Personalized PageRank\n",
    "            personalization = [0] * len(g.vs)\n",
    "            # Set personalization vector \n",
    "            personalization_value  = 1.0 / len(nodes)\n",
    "            for node in nodes:\n",
    "                idx = g.vs.find(name=str(node[\"id\"])).index\n",
    "                personalization[idx] = personalization_value\n",
    "            #https://igraph.org/python/api/0.9.11/igraph._igraph.GraphBase.html#personalized_pagerank\n",
    "            pagerank_scores = g.personalized_pagerank(damping=0.85, reset=personalization)\n",
    "            # print(pagerank_scores)\n",
    "            # print(g.vs[\"passageId\"])\n",
    "            # get sorted nodes by pagerank descending\n",
    "            pagerank_nodes = sorted(zip(g.vs[\"passageId\"], pagerank_scores), key=lambda x: x[1], reverse=True)\n",
    "            # print(pagerank_nodes)\n",
    "\n",
    "            for node in pagerank_nodes:\n",
    "                if node[1] > 0:\n",
    "                    print(node)\n",
    "                    passage_ids.append(node[0])\n",
    "\n",
    "    return passage_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query entities: {'entities': ['modelos de casas de vegetação capela', 'dente-de-serra'], 'named_entities': ['modelos de casas de vegetação capela', 'dente-de-serra']}\n",
      "(0, 0.2622475048242614)\n",
      "(1, 0.2622475048242614)\n",
      "(1, 0.1114551895503111)\n",
      "(1, 0.1114551895503111)\n",
      "(5, 0.04458207582012444)\n",
      "(3, 0.02229103791006222)\n",
      "(0, 0.02229103791006222)\n",
      "(0, 0.02229103791006222)\n",
      "(5, 0.02229103791006222)\n",
      "(5, 0.02229103791006222)\n",
      "(5, 0.02229103791006222)\n",
      "(5, 0.02229103791006222)\n",
      "(5, 0.02229103791006222)\n",
      "(2, 0.0063157940745176295)\n",
      "(3, 0.0063157940745176295)\n",
      "(3, 0.0063157940745176295)\n",
      "(2, 0.005368424963339984)\n",
      "(3, 0.002684212481669992)\n",
      "(3, 0.002684212481669992)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Os modelos de casas de vegetação capela e dente-de-serra possuem diferenças significativas em termos de estrutura e adequação climática:\\n\\n1. **Modelo Capela**:\\n   - **Estrutura**: Este modelo é assim chamado pela sua semelhança com uma capela, com telhado de duas águas iguais.\\n   - **Adequação Climática**: É um modelo de uso generalizado nas cinco regiões geográficas do país, mas sua melhor utilização é em condições de clima quente e úmido (Centro-Oeste e Sudeste), quente e seco (Nordeste) e equatorial, como o da Amazônia.\\n\\n2. **Modelo Dente-de-Serra**:\\n   - **Estrutura**: Tem esse nome porque o perfil transversal deste modelo lembra a figura de dentes de serra.\\n   - **Adequação Climática**: É indicado para condições de elevada temperatura e umidade, como as condições climáticas predominantes nas Regiões Sudeste, Centro-Oeste, Nordeste e Amazônica.\\n\\nEssas diferenças refletem a adaptação de cada modelo às condições climáticas específicas, visando otimizar o cultivo hidropônico em diferentes regiões.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the final answer with the passages\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "\n",
    "# User question\n",
    "user_query = \"Quais as diferenças entre os modelos de casas de vegetação capela e dente-de-serra?\"\n",
    "# query = \"Qual a capacidade do tanque de combustível da duster?\"\n",
    "\n",
    "# Extract name entities from query\n",
    "query_entities = extract_entities_from_query(user_query)\n",
    "print(\"Query entities:\",query_entities)\n",
    "\n",
    "# find the nodes on the graphDB that are similar to the named entities in the query\n",
    "base_nodes = []\n",
    "if len(query_entities[\"named_entities\"]) == 0:\n",
    "    print(\"No named entities found in the query\")\n",
    "else:\n",
    "    for query_entitie in query_entities[\"named_entities\"]:\n",
    "        results = vector_search_graphdb(query_entitie)\n",
    "        base_nodes.extend(results)\n",
    "\n",
    "# get the passages ids that are \"good\" nodes\n",
    "result_passages_ids = ppr_graphdb(base_nodes)\n",
    "\n",
    "# create the context with the passages \n",
    "context = \"\"\n",
    "unique_passages_id = list(set(result_passages_ids))\n",
    "for passage_id in unique_passages_id:\n",
    "    for passage in passages_arr:\n",
    "        if passage[\"id\"] == passage_id:\n",
    "            context += passage[\"text\"] + \"\\n\\n\"\n",
    "            break\n",
    "\n",
    "template = \"\"\"Answer the question based only on the provided context, answer in the same languague as the question.\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "rag_prompt_text = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Answer the question based only on the provided context, answer in the same languague as the question.\"),\n",
    "        (\"human\", \"\"\"Context:```\n",
    "{context}\n",
    "```\n",
    "\n",
    "Question: {question}\"\"\"),\n",
    "    ]\n",
    ")\n",
    "# Create the chain to answer the question, will use a stronger model like GPT-4o\n",
    "final_chain = rag_prompt_text | ChatOpenAI(model=\"gpt-4o\", temperature=0.0) | StrOutputParser()\n",
    "\n",
    "# Invoke the chain with a query\n",
    "answer = final_chain.invoke({\"context\": context, \"question\": user_query})\n",
    "\n",
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ircot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
